{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805c58a4-d7b4-4ea9-95fc-2dcd32ce235a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emil\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import regex as re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590af661-775b-430e-a68a-ac113159eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffea9030-30c9-4fed-844d-d31bb561a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = \"KB/bert-base-swedish-cased\" #Options: KB/bert-base-swedish-cased, bert-base-uncased\n",
    "reg = r'\\p{L}+|-|\\*'\n",
    "max_length = 100\n",
    "batch_size = 16\n",
    "lr = 3e-5\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e3be76-832e-4982-9ce3-13153d01c5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011\n"
     ]
    }
   ],
   "source": [
    "with open(\"train_data_our.json\", encoding = \"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "print(len(data))\n",
    "split = (int) (len(data) * 0.8)\n",
    "train_data = data[:split]\n",
    "test_data = data[split:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5bc4c55-a07c-435b-aae2-926263d57e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'A', '.', '[UNK]', '.', 'vul', '##g', '.', ',', 'förkortning', 'för', 'Ann', '##o', '[UNK]', 'vul', '##gar', '##is', '(', 'lat', '.', ')', ',', 'året', 'efter', 'den', 'vanliga', 'tid', '##räkningen', '.', '[UNK]', '[UNK]', '«', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "A. ær. vulg.\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(bert)\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "regtest = r'\\p{L}+|[\\p{P}]|[\\p{N}]+|[^\\p{L}\\s]+'\n",
    "\n",
    "\n",
    "for data in train_data:\n",
    "    inputs.append(tokenizer(re.findall(regtest,data[0]),return_tensors=\"pt\",padding = 'max_length',max_length=max_length,truncation=True,is_split_into_words=True))\n",
    "\n",
    "    \n",
    "for i in range(len(inputs)):\n",
    "    hword = tokenizer(re.findall(regtest,train_data[i][1]),return_tensors=\"pt\",padding = 'max_length',max_length=20,truncation=True,is_split_into_words=True)\n",
    "    hword = hword['input_ids'].tolist()[0] \n",
    "    hword = hword[1:hword.index(3)]\n",
    "    words = inputs[i]['input_ids'].tolist()[0]\n",
    "\n",
    "    \n",
    "    start = -1\n",
    "    end = -1\n",
    "\n",
    "    for j in range(len(words)):\n",
    "        if words[j:j+len(hword)] == hword:\n",
    "            start = j\n",
    "            end = j + len(hword)\n",
    "            break;\n",
    "\n",
    "    label = [0] * max_length\n",
    "    label[start:end] = [1] * (end-start)\n",
    "    \n",
    "    labels.append(torch.tensor(label))\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for input_id in inputs:\n",
    "    input_ids.append(input_id['input_ids'].squeeze(0))\n",
    "    attention_masks.append(input_id['attention_mask'].squeeze(0))\n",
    "for i in range(len(input_ids)):\n",
    "    print(tokenizer.convert_ids_to_tokens(inputs[i]['input_ids'][0]))\n",
    "    print(labels[i])\n",
    "    print(train_data[i][1])\n",
    "\n",
    "\n",
    "input_ids = torch.stack(input_ids)\n",
    "labels = torch.stack(labels)\n",
    "attention_masks = torch.stack(attention_masks)\n",
    "\n",
    "print(input_ids.size())\n",
    "print(labels.size())\n",
    "print(attention_masks.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40ce7f8-2975-4b3e-b16b-f606e120c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids,labels,attention_masks)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cffd3570-f524-4253-b056-3d6deaff2867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(bert, num_labels=2).to(device) #KB/bert-base-swedish-cased bert-base-uncased\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "19729503-bb7e-407d-9815-f3df7f2c2781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:05<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:05<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:05<00:00, 14.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:05<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:05<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:05<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:05<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:05<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:05<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:05<00:00, 14.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs,target,attention_mask in tqdm(dataloader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        #Forward\n",
    "        outputs = model(input_ids=inputs,labels=target,attention_mask = attention_mask)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        #Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b129332d-d0af-4b0b-a74f-6b77b040e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"annotatorActive.pt\") # ALDRIG DEN HÄRRRRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10a7186-aaf8-4c7c-bcd5-1febca3060e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50325, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(bert, num_labels=2).to(device)\n",
    "model.load_state_dict(torch.load(\"annotatorActive.pt\",weights_only = True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bee5bb9-5edb-4ed5-a528-a88cb06a15a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 89/302 [00:03<00:03, 65.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative\n",
      "\n",
      "Norskkonst\n",
      "Norsk konst framträder under den äldsta kristna tiden\n",
      "hufvudsakligen som arkitektur. Stenbyggnadskonst\n",
      "infördes af angelsaxiska och iriska präster\n",
      "och munkar. De äldsta stenkyrkorna voro ganska\n",
      "små, rektangulära, enskeppiga. Ett lägre och\n",
      "smalare kor med rätlinig afslutning fogades\n",
      "ibland till kyrkan? skepp, och denna täcktes med\n",
      "ett spärr-tak. Denna irisk-angelsaziska period\n",
      "aflöses under senare hälften af 1000-talet af den\n",
      "anglonorman-diska perioden. De större kyrkorna byggdes\n",
      "nu oftast treskeppiga med halfcirkelformigt eller\n",
      "linje-rät korafslutning samt hvälfda. Bland kyrkor\n",
      "från denna tid märkas Gamle Åkers kyrka i Kristiania,\n",
      "kyrkor i Gran, Ringsaker, ruinen af Hamars domkyrka,\n",
      "Mariakyrkan i Bergen, skeppet i Stavangers domkyrka,\n",
      "kyrkor i Hove, Ysernes, Msere samt tvärhuset i\n",
      "Trondhjems domkyrka. Från detta skede kvarstå ruiner\n",
      "af flera kloster, bl. a. på Hovedöen vid Oslo. Vid\n",
      "slutet af 1100-talet börja öfvergångs-former till\n",
      "gotisk stil göra sig gällande, och vid midten af\n",
      "1200-talet har gotiken nått sin blomstring, också\n",
      "den under starkt engelskt inflytande. Dess\n",
      "\n",
      "False Negative\n",
      "\n",
      "Stockholmslän\n",
      "Stockholms län omfattar östra delarna af Uppland och Södermanland och är (hvad fastlandet beträffar) beläget mellan 58° 48’ och 60° 26’ n. br. samt 0° 45’ v. lgd och 1° 2’ ö. lgd från Stockholms observatorium. Det gränsar i n. till Bottniska viken, i ö. till Södra Kvarken, Ålands haf och Östersjön, i s. till Östersjön och i v. till Svärds- och Himmerfjärdarna, Södermanlands län, Mälaren och Uppsala län. Ytinnehållet är, sedan Brännkyrka och Bromma socknar med tillsammans 105 kvkm. vid början af resp. 1913 och 1916 inkorporerats i Stockholms stad, 7,739 kvkm., däraf 7,390 kvkm. land och 349 kvkm. vatten. I landets areal äro inräknade närmare 1,500 kvkm. öar och holmar, bland hvilka de största äro, i Östersjön Värmdö med Ingarö (243 kvkm.), Väddö och Björkö (125 kvkm.) och Gräsö (88 kvkm.) samt i Mälaren Svartsjölandet (79 kvkm.) och Ekerön med Munsön (66 kvkm.). I vattnets areal ingår icke Mälaren, ej heller från hafvet inskjutande vikar och sund. Länets största längd, från n. till s., är nära 200 km. och största bredd, fr. v. till ö., 100 km. Folkmängden vid slutet af 1916 var 225,731 personer, hvaraf 111,023 mankön, 114,708 kvinnkön. Landsbygdens folkmängd var 197,509, hvaraf 45,382 i köpingar och municipalsamhällen, städernas 28,222. Folkmängdstätheten utgjorde 31 på 1 kvkm. land. Folkökningen har under de sista årtiondena varit synnerligen stark, 1897–1906 24,3 proc., 1907–16 13,8 proc. Sistnämnda jämförelsevis låga siffra beror på den nyssnämnda inkorporeringen af Brännkyrka och Bromma i Stockholms stad; frånräknas dessa från länet jämväl för 1906, stiger folkökningen 1907–16 till 23,5 proc. Tillsammans har under de 20 åren 1897–1916 folkmängden i länet, utom Brännkyrka och Bromma, ökats med 48,7 proc., hvilket är mera än i något annat län (Norrbottens län 48,6 proc., Stockholms stad 46,1 proc. eller, om Brännkyrka och Bromma frånräknas 1916, endast 32,5 proc.). I judiciellt hänseende är länet deladt i 6 domsagor: Stockholms läns västra (1 tingslag: Långhundra, Ärlinghundra, Seminghundra och Vallentuna härads), Norra Roslags (från 1 jan. 1918 2 t:g: Väddö och Närdinghundra samt Frösåkers), Mellersta Roslags (1 t:g: Lyhundra och Sjuhundra härads och Bro och Vätö samt Frötuna och Länna skeppslags), Sollentuna och Färentuna härads (1 t:g), Södra Roslags (1 t:g: Danderyds, Åkers och Värmdö skeppslags) samt Södertörns (2 t:g: Sotholms samt Svartlösa och Öknebo). Tingslagens antal är alltså 8. Fögderierna äro från 1 jan. 1918 6: Norra Roslags (motsvarande Norra Roslags domsaga), Mellersta Roslags (motsv. Mellersta Roslags och en del af Stockholms läns västra d:a), Svartsjö (motsv. Sollentuna och Färentuna härads samt en del af Stockholms läns västra d:a), Danderyds (motsv. en del af Södra Roslags d:a), Värmdö (motsv. delar af Stockholms läns västra, Södra Roslags och Södertörns domsagor) samt Södertörns (motsv. en del af Södertörns d:a). Landsfiskalsdistrikten äro från samma tidpunkt 21, nämligen 5 i Värmdö, 4 i Mellersta Roslags och i Södertörns, 3 i Norra Roslags och i Svartsjö samt 2 i Danderyds fögderi. Städerna äro 7: Södertälje, Vaxholm, Norrtälje, Östhammar, Öregrund och Sigtuna, samtliga med rådhusrätt (ehuru Östhammar och Öregrund ha gemensam borgmästare), samt Djursholm, som lyder under landsrätt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 302/302 [00:06<00:00, 48.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive:  0.9271523178807947\n",
      "True Negative:  0.059602649006622516\n",
      "False Positive:  0.006622516556291391\n",
      "False Negative:  0.006622516556291391\n",
      "Precision:  0.9929078014184397 ,  Recall:  0.9929078014184397 ,  Accuracy:  0.9867549668874173 ,  F1:  0.9929078014184397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Validation\n",
    "tp = 0\n",
    "tf = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "c= 0\n",
    "\n",
    "extractlist = []\n",
    "\n",
    "for desc, headword in tqdm(test_data):\n",
    "    last = -1\n",
    "    converter = tokenizer(re.findall(regtest,desc),return_tensors=\"pt\",padding = 'max_length',max_length=max_length,truncation=True,is_split_into_words=True)\n",
    "    input_ids = converter[\"input_ids\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids = input_ids, attention_mask = converter['attention_mask'].to(device))\n",
    "\n",
    "    probs = F.softmax(outputs.logits, dim=-1)\n",
    "    preds = torch.argmax(probs, dim=-1)\n",
    "    predlist = preds.tolist()\n",
    "    i = 0\n",
    "    word = \"\"\n",
    "\n",
    "    while i < len(converter.word_ids()):\n",
    "        \n",
    "        if predlist[0][i] == 1:\n",
    "            if converter.word_ids()[i] != last and converter.word_ids()[i] != None:\n",
    "                word = word + \" \" + re.findall(regtest,desc)[converter.word_ids()[i]]\n",
    "            \n",
    "            last = converter.word_ids()[i]\n",
    "    \n",
    "        i +=1\n",
    "\n",
    "\n",
    "    word = re.findall(reg,word)\n",
    "    extractlist.append([word,desc])\n",
    "    \n",
    "    headword = headword.replace(\" \",\"\")\n",
    "    wordtemp = \"\"\n",
    "    if len(word)>0:\n",
    "        for w in word:\n",
    "            wordtemp += w\n",
    "            \n",
    "    word = wordtemp\n",
    "\n",
    "    \n",
    "    if word == headword:\n",
    "        tp += 1\n",
    "    elif len(word) > 0 and len(headword) == 0:\n",
    "        fp += 1\n",
    "    elif len(word) == 0 and len(headword) > 0:\n",
    "        print(\"False Negative\")\n",
    "        print(word)\n",
    "        print(headword)\n",
    "        print(desc)\n",
    "        fn += 1\n",
    "    else:\n",
    "        tf += 1\n",
    "\n",
    "print(\"True Positive: \", tp/len(test_data))\n",
    "print(\"True Negative: \", tf/len(test_data))\n",
    "print(\"False Positive: \", fp/len(test_data))\n",
    "print(\"False Negative: \", fn/len(test_data))\n",
    "precision = tp/(tp+fp)\n",
    "recall = (tp)/(tp +fn)\n",
    "print(\"Precision: \", tp/(tp+fp),\", \",\"Recall: \",(tp)/(tp +fn),\", \",\"Accuracy: \",(tp +tf)/len(test_data),\", \",\"F1: \", 2* ((precision * recall)/(precision+recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa596fb-768c-4663-ab81-59bb45a013ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249550/249550 [56:06<00:00, 74.12it/s] \n"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "editionV = \"U3_E.\"\n",
    "v = 1\n",
    "\n",
    "with open(\"TREDJE_PARAGRAPHS.json\", encoding = \"utf-8\") as f:\n",
    "    edition = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "edition2 = []\n",
    "for i in range (len(edition)):\n",
    "    edition2.append(edition[i]['paragraph'])\n",
    "\n",
    "extractlist = []\n",
    "\n",
    "\n",
    "for desc in tqdm(edition2):\n",
    "    \n",
    "    converter = tokenizer(re.findall(regtest,desc),return_tensors=\"pt\",padding = 'max_length',max_length=max_length,truncation=True,is_split_into_words=True)\n",
    "    input_ids = converter[\"input_ids\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids = input_ids, attention_mask = converter['attention_mask'].to(device))\n",
    "\n",
    "    probs = F.softmax(outputs.logits, dim=-1)\n",
    "    preds = torch.argmax(probs, dim=-1)\n",
    "    predlist = preds.tolist()\n",
    "    \n",
    "    i = 0\n",
    "    word = []\n",
    "\n",
    "    input_list = input_ids.tolist()[0]\n",
    "\n",
    "    for k in range(len(predlist[0])):\n",
    "        if predlist[0][k] == 1:\n",
    "            word.append(input_list[k])\n",
    "\n",
    "    regspace = r'\\s'\n",
    "\n",
    "    spaces = re.finditer(regspace,desc)\n",
    "    descNoSpaces = desc.replace(\" \",\"\")\n",
    "    spaceInd = []\n",
    "\n",
    "    for match in spaces:\n",
    "        spaceInd.append(match.span()[0])\n",
    "\n",
    "    word = \"\"\n",
    "    last = \"\"\n",
    "    while i < len(converter.word_ids()):\n",
    "        \n",
    "        if predlist[0][i] == 1:\n",
    "            if converter.word_ids()[i] != last and converter.word_ids()[i] != None:\n",
    "                word = word + re.findall(regtest,desc)[converter.word_ids()[i]]\n",
    "            \n",
    "            last = converter.word_ids()[i]\n",
    "    \n",
    "        i +=1\n",
    "\n",
    "    \n",
    "\n",
    "    if len(word) > 0:\n",
    "        found = descNoSpaces.find(word)\n",
    "        if found != -1:\n",
    "            temp = spaceInd\n",
    "            for t in temp:\n",
    "                if t < found:\n",
    "                    del spaceInd[0]\n",
    "            for s in spaceInd:\n",
    "                if s > len(word):\n",
    "                    break;\n",
    "                word = word[:s-found] + \" \" + word[s-found:]\n",
    "            \n",
    "            if word[-1] == \" \":\n",
    "                word = word[:-1]\n",
    "            extractlist.append({\"id\":editionV.replace(\".\",str(v)),\"headword\":word,\"description\":desc})\n",
    "            v += 1\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "with open (\"TREDJE_EXTRACTED.json\",\"w\",encoding = \"utf-8\") as f:\n",
    "    json.dump(extractlist,f,indent = 4,ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90966602-4501-4079-9648-8a41ef3c2482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124158\n"
     ]
    }
   ],
   "source": [
    "print(len(extractlist))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
